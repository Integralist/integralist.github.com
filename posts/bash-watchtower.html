<!doctype html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, minimal-ui">
        <title>Bash Watchtower</title>
        <link rel="stylesheet" href="../styles/github-markdown.css">
        <link rel="stylesheet" href="../styles/prism-default.css">
        <style>
            body {
                min-width: 200px;
                max-width: 790px;
                margin: 0 auto;
                padding: 30px;
            }

            pre[class*="language-"] {
              border: 1px dashed #AAA;
              margin-bottom: 1em;
            }
        </style>
    </head>
    <body>
        <article class="markdown-body">
          <ul class="top-nav">
              <li><a href="../index.html">Home</a></li>
              <li><a href="../posts/about.html">About Me</a></li>
              <li><a href="https://github.com/integralist">GitHub</a></li>
              <li><a href="https://twitter.com/integralist">Twitter</a></li>
              <li><a href="http://www.integralist.co.uk/resume">Resume</a></li>
          </ul>

<h1>Bash Watchtower</h1>

<ul>
<li><a href="#1">Introduction</a></li>
<li><a href="#2">How does it work?</a></li>
<li><a href="#3">Comparison</a></li>
<li><a href="#4">Code</a></li>
<li><a href="#5">Explanation</a></li>
<li><a href="#6">Conclusion</a></li>
</ul>

<div id="1"></div>

<h2>Introduction</h2>

<p>This is a quick post to demonstrate how I use a simple <a href="https://www.gnu.org/software/bash/">Bash</a> shell script to report when web pages are failing (e.g. returning a non-200 HTTP status code). It does this by sending notifications of the URL which returned a non-200 status code into a remote application (in my case <a href="https://slack.com/">Slack</a>; but you could modify the script to suit whatever service you happen to be using).</p>

<p>I run this script via <a href="https://jenkins-ci.org/">Jenkins CI</a> on a five minute cron. The inspiration came from <a href="https://twitter.com/charlierevett">Charlie Revett</a> who wrote a <a href="https://nodejs.org/">nodejs</a> package called <a href="http://github.com/revett/watchtower/">Watchtower</a>. I like shell scripts (not so much Node) and so I decided, for no real good reason, to replicate his package in Bash.</p>

<div id="2"></div>

<h2>How does it work?</h2>

<p>The script has the following steps:</p>

<ol>
<li>Cleanup: remove any temporary files created during a previous run</li>
<li>Retrieve: curl the remote endpoints in parallel</li>
<li>Notify: parse the responses and send notification for any that fail</li>
</ol>

<div id="3"></div>

<h2>Comparison</h2>

<p>Well, the Node package has quite a few layers to it (e.g. Dockerfile, package.json, dependencies, multiple nested files that take some time to navigate around) where as my &#39;Bash Watchtower&#39; is a single shell script. So it&#39;s actually a lot easier and quicker (in my opinion at least) to understand what&#39;s going on and how things work.</p>

<blockquote>
<p>Note: on the plus side, he&#39;s got tests :-)<br>
I couldn&#39;t be bothered with that for this quick hack</p>
</blockquote>

<p>My initial concern was going to be around the performance of requesting multiple endpoints, as well as sending potentially multiple failure notifications to the remote service (Slack). I knew that Node is popular for its event driven concurrency, and I was keen to ensure performance wasn&#39;t degraded in any way. </p>

<p>I&#39;d argue (in theory, I haven&#39;t actually tested) that performance would be equal or better because I&#39;m running the relevant sections of the code in <em>parallel</em> rather than <em>concurrently</em> using the shell&#39;s <code>&amp;</code> operator to &#39;background&#39; each request/notification into a separate subshell. I&#39;m then utilising the <code>wait</code> command which (as the name suggests) waits for all currently active child processes to complete.</p>

<blockquote>
<p>Note: because of the background processes, this script will not scale and be as performant once the number of URLs you&#39;re looking to check against becomes very large. So if you&#39;re looking to validate 100&#39;s of URLs, then you&#39;ll likely hit performance issues</p>
</blockquote>

<div id="4"></div>

<h2>Code</h2>

<p>So here is the code:</p>

<pre><code class="language-bash">function cleanup() {
  rm results.txt
  rm temp.txt
}

function pull() {
  local base=$1
  local urls=(&quot;${!2}&quot;)

  for resource in &quot;${urls[@]}&quot;
  do
    curl $base$resource --head \
                        --location \
                        --silent \
                        --output /dev/null \
                        --connect-timeout 2 \
                        --write-out &quot;%{url_effective} %{http_code}\n&quot; &amp;
  done

  wait
}

function parse() {
  local results=$1
  local remote=https://hooks.slack.com/services/foo/bar/baz

  cat $results | awk &#39;!/200/ { print $2 &quot;: &quot; $1 }&#39; &gt; temp.txt

  while read line; do
    curl --header &quot;Content-Type: application/json&quot; \
         --silent \
         --output /dev/null \
         --request POST \
         --data &quot;{\&quot;text\&quot;: \&quot;$line\&quot;}&quot; $remote &amp;
  done &lt; temp.txt

  wait

  display temp.txt
}

function display() {
  printf &quot;\n\n&quot;
  cat $1
  printf &quot;\n\n&quot;
}

trap cleanup EXIT

endpoints=(
  /newsbeat
  /newsbeat/popular
  /newsbeat/topics
  /newsbeat/topics/entertainment
  /newsbeat/topics/surgery
  /newsbeat/article/32792353/im-engaged-but-will-i-ever-be-able-to-marry-my-boyfriend
)

pull http://bbc.co.uk endpoints[@] &gt; results.txt
display results.txt
parse results.txt
</code></pre>

<blockquote>
<p>Note: I&#39;ve multilined the <code>curl</code> request here for readability (but I prefer one liners)</p>
</blockquote>

<div id="5"></div>

<h2>Explanation</h2>

<p>The script is broken out into functions:</p>

<ul>
<li><code>cleanup</code>: removes specified files</li>
<li><code>pull</code>: gets our endpoints (only the HTTP headers)</li>
<li><code>parse</code>: looks for non-200 status code and sends notification</li>
<li><code>display</code>: prints specified file</li>
</ul>

<p>The <code>cleanup</code> and <code>display</code> functions aren&#39;t of any special interest, so we&#39;ll focus primarily on <code>pull</code> and <code>parse</code>. The only thing I will say is that previously I was manually calling <code>cleanup</code> twice (the function was originally written to take an argument - a file path - and remove the specified file if it indeed existed); this has since changed to not take an argument but instead explictly remove the two files I know I create within this script.</p>

<p>I also now automatically run the <code>cleanup</code> function when the shell exits. I do this using:</p>

<pre><code class="language-bash">trap cleanup EXIT
</code></pre>

<p>If you&#39;ve not seen this before then please refer to <code>help trap</code> for more details.</p>

<blockquote>
<p>Note: most of the time the <code>man &lt;command&gt;</code> will help you locate information<br>
But with builtin commands (those that are part of the shell environment itself)<br>
you need to use: <code>help &lt;command&gt;</code> (e.g. <code>help trap</code> or <code>help wait</code>)<br>
Failing that you could search inside <code>man bash</code> but that&#39;s lunacy! </p>
</blockquote>

<h3>Pull</h3>

<p>First we take in two arguments, the first we store in a local variable called <code>base</code> while the other is stored in a variable called <code>urls</code>. You&#39;ll notice we&#39;ve had to convert the second argument into an Array by assigning something that resembles an Array (e.g. the parentheses <code>(...)</code>) and then expand the incoming string of elements inside it (<code>(&quot;${!2}&quot;)</code>).</p>

<blockquote>
<p>Note: you&#39;ll notice that when we call <code>pull</code><br>
we have to pass <code>endpoints[@]</code> and not <code>$endpoints</code><br>
this is to ensure we properly expand all elements within the Array</p>
</blockquote>

<p>Next we loop over the <code>urls</code> Array and for each item we send a <code>curl</code> request (which in this case is a unique URL constructed from the <code>$base</code> and <code>$resource</code> variables), but we specify that we&#39;re only interested in getting back the HTTP headers for the request (<code>--head</code>).</p>

<p>We make sure that if the resource being requested actually <code>301</code> redirects to another endpoint, then we should follow that redirect to the new location (<code>--location</code>). We&#39;re also not interested in any progress bars or error output (<code>--silent</code>). We direct any other &#39;output&#39; to <code>/dev/null</code>, as we don&#39;t need it (<code>--output /dev/null</code>).</p>

<p>After this we specify a timeout for each request, as we don&#39;t want a slow server to impact our script&#39;s performance (<code>--connect-timeout 2</code>). Now we tell <code>curl</code> to make sure after a successful request it should dump out some additional information to <code>stdout</code> and that it should be formatted in a specific way (<code>--write-out &quot;%{url_effective} %{http_code}</code>) as this makes it easier for us to deal with (as outside of this function we redirect this <code>stdout</code> information into a file called <code>result.txt</code>).</p>

<p>Finally we call <code>wait</code>, which as we now know (see above) will wait for each of the backgrounded child processes to complete before the function ends.</p>

<h3>Parse</h3>

<p>In this function we take in a single argument, the <code>results.txt</code> file, which would contain a set of &#39;results&#39; that <em>could</em> look something like:</p>

<pre><code class="language-bash">http://www.bbc.co.uk/newsbeat/topics/entertainment 200
http://www.bbc.co.uk/newsbeat/popular 200
http://www.bbc.co.uk/newsbeat/topics 200
http://www.bbc.co.uk/newsbeat 200
http://www.bbc.co.uk/newsbeat/topics/surgery 200
http://www.bbc.co.uk/newsbeat/article/32792353/im-engaged-but-will-i-ever-be-able-to-marry-my-boyfriend 500
</code></pre>

<blockquote>
<p>Note: here the results suggest only one URL has returned a 500 status code</p>
</blockquote>

<p>We also store off our remote endpoint (in my case: our Slack incoming webhook URL) in a variable called <code>remote</code>. This is where we&#39;ll be sending our JSON data of failed URLs to.</p>

<p>At this point we use <a href="https://en.wikipedia.org/wiki/AWK">Awk</a> to check each line of the incoming <code>results.txt</code> to see if it doesn&#39;t include <code>200</code> somewhere. If it doesn&#39;t then we store that line into a <code>temp.txt</code> file in the format of <code>&lt;status_code&gt; &lt;url&gt;</code>. We then redirect the contents of <code>temp.txt</code> into a <code>while read</code> loop and for each line we <code>curl</code> our remote endpoint (in parallel using <code>&amp;</code>); POST&#39;ing it a JSON object that details the URL that gave a non-200 response.</p>

<p>Again, like the <code>pull</code> function, we utilise <code>wait</code> to ensure all the child subprocesses finish before doing some final displaying and cleanup of the <code>temp.txt</code> file and then returning the function back to the caller.</p>

<div id="6"></div>

<h2>Conclusion</h2>

<p>That&#39;s it. Fairly standard Bash scripting. I&#39;m sure they&#39;ll be some unix/linux neck-beard wizards in the audience ready to &#39;shred me a new one&#39; because my chops aren&#39;t as <em>wizardy</em> as theirs. If that&#39;s the case: feel free to get in contact as I&#39;d love to know how I could make this code simpler or easier to work with (or just more idiomatic).</p>

            <hr>
            <h2>Links</h2>
            <ul>
                <li><a href="../index.html">Home</a></li>
                <li><a href="../posts/about.html">About Me</a></li>
                <li><a href="https://github.com/integralist">GitHub</a></li>
                <li><a href="https://twitter.com/integralist">Twitter</a></li>
                <li><a href="http://www.integralist.co.uk/resume">Resume</a></li>
            </ul>
        </article>
        <script src="../scripts/prism.js"></script>
        <script>
            var _gaq=[['_setAccount','UA-33159515-1'],['_trackPageview']];
            (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
            g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
            s.parentNode.insertBefore(g,s)}(document,'script'));
        </script>
    </body>
</html>
